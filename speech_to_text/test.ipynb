{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring the Environment\n",
    "\n",
    "Steps\n",
    "\n",
    "* ensure working environment\n",
    "* convert remote .wav, .mp3 into dataset with metadata, [ref](https://huggingface.co/docs/datasets/audio_load)\n",
    "  - create `metadata.csv`\n",
    "```\n",
    "samples/\n",
    "├── README.md\n",
    "├── loader.py\n",
    "├── metadata.csv\n",
    "├── raw/      #.wav, .mp3\n",
    "└── data/     #.tar.gz\n",
    "```\n",
    "* create loading script\n",
    "  - `Audio datasets are commonly stored in tar.gz archives which requires a particular approach to support streaming mode`\n",
    "* individually stream pipeline, [ref](https://huggingface.co/docs/datasets/stream)\n",
    "```\n",
    ">>> from datasets import load_dataset\n",
    ">>> #NO SLOW: dataset = load_dataset('oscar-corpus/OSCAR-2201', 'en', split='train', streaming=True)\n",
    ">>> dataset = load_dataset(\"food101\")\n",
    ">>> iterable_dataset = dataset.to_iterable_dataset()\n",
    ">>> print(next(iter(iterable_dataset)))\n",
    "```\n",
    "* run from dataset\n",
    "  - resample the sampling rate, [ref](https://huggingface.co/docs/datasets/audio_process)\n",
    "  - preprocess, `map()`\n",
    "  - diarize process\n",
    "  - extract data as timeline\n",
    "  - apply classification models\n",
    "* format as individual .pdf\n",
    "* format as vdi workspace\n",
    "* ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Model Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual steps with internet\n",
    "\n",
    "We use internet to get the model and save the pretrained model and tokenizer to local directory. Then you can zip and download it for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0cff82520f4b49b710e33fb4835c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3c7bdff50a41148da59aac7a830501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c3cc49eace4a5ca110d5afd24fa96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfaf182ef6a64855aabfafbbe7d0fc92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693332ff14824eba86533b3f5698338f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a15e31e2854b699b97823ea50d3136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752421563dbf4d528bce7b2e5f93ebcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2eed4edabd48289f1bf60d8319a633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6ec0bbc49d417485a97ae399be0d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6636cb24913241f88c8d6d43800a8ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9bf7a5801a49479bfa2bbbfd82ca7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/3.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': '/home/vscode/.cache/huggingface/datasets/downloads/extracted/69d899cdf280bc629c9d8609fa18cea800a77bd64686d98ea020dddf62fd77a3/dev_clean/1272/128104/1272-128104-0000.flac',\n",
       " 'audio': {'path': '/home/vscode/.cache/huggingface/datasets/downloads/extracted/69d899cdf280bc629c9d8609fa18cea800a77bd64686d98ea020dddf62fd77a3/dev_clean/1272/128104/1272-128104-0000.flac',\n",
       "  'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
       "         0.0010376 ]),\n",
       "  'sampling_rate': 16000},\n",
       " 'text': 'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL',\n",
       " 'speaker_id': 1272,\n",
       " 'chapter_id': 128104,\n",
       " 'id': '1272-128104-0000'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec150e49b1f74de8bb78ac5ec2508e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset librispeech_asr_dummy/clean to /home/vscode/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b252e4b96248f0bb607011e2fb0440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3b4442983242baaee1f089e7b6f37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f62704a0d2646d697813819c9c17ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1f7f183b074276b5370d3483d0b63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset librispeech_asr_dummy downloaded and prepared to /home/vscode/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.<|endoftext|>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "import warnings, logging\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download & load the model from HuggingFace modelhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5319fd3aa0a446f0830d642a6ea23a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/578 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e49cd47a1b433dabc88d60c44540eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/273M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d4cf387dae41018bfad8e862ae9ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecc2f58514d4ec993925ab1e6adfc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_nm = 'microsoft/deberta-v3-small'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, return_dict=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'deberta_v3_small_pretrained_model_pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {save_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('deberta_v3_small_pretrained_model_pytorch/tokenizer_config.json',\n",
       " 'deberta_v3_small_pretrained_model_pytorch/special_tokens_map.json',\n",
       " 'deberta_v3_small_pretrained_model_pytorch/spm.model',\n",
       " 'deberta_v3_small_pretrained_model_pytorch/added_tokens.json',\n",
       " 'deberta_v3_small_pretrained_model_pytorch/tokenizer.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added_tokens.json  special_tokens_map.json  tokenizer_config.json\r\n",
      "config.json\t   spm.model\r\n",
      "pytorch_model.bin  tokenizer.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls {save_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading from saved path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(save_path, return_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add to archive to download locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: deberta_v3_small_pretrained_model_pytorch/ (stored 0%)\r\n",
      "  adding: deberta_v3_small_pretrained_model_pytorch/tokenizer.json (deflated 77%)\r\n",
      "  adding: deberta_v3_small_pretrained_model_pytorch/spm.model (deflated 50%)\r\n",
      "  adding: deberta_v3_small_pretrained_model_pytorch/config.json (deflated 53%)\r\n",
      "  adding: deberta_v3_small_pretrained_model_pytorch/special_tokens_map.json (deflated 54%)\r\n",
      "  adding: deberta_v3_small_pretrained_model_pytorch/pytorch_model.bin (deflated 42%)\r\n",
      "  adding: deberta_v3_small_pretrained_model_pytorch/added_tokens.json (stored 0%)\r\n",
      "  adding: deberta_v3_small_pretrained_model_pytorch/tokenizer_config.json (deflated 45%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r debertav3_small.zip {save_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing redundant files\n",
    "!rm -rf {save_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual steps without internet\n",
    "\n",
    "* Search the model you like on https://huggingface.co/spaces/huggingface-projects/diffusers-gallery\n",
    "* The files in a huggingface repo / `Files and versions` required to run the model - weights, tokenizers, configurations, etc.\n",
    "* In `Files and versions`, search for file that end with `.ckpt` or `.safetensors`, press down arrow to download it. Then just place it in `model/Stable-diffusion` folder just like when you download from civitai\n",
    "  - The [`Safetensors`](https://github.com/huggingface/safetensors) format is a relatively new data serialization format that is being developed by HuggingFace. It has many advantages over the ckpt format, including: \n",
    "    + Faster loading times in various ML applications (on both CPU and GPU)\n",
    "    + Cross-platform compatibility (It’s not specifically designed for Python like Pickle)\n",
    "    + Safety (Does not make use of pickle serialization method which can allow for remote code execution)\n",
    "  - there is no difference between `.ckpt` and `.pth` \n",
    "    + A CKPT file is a checkpoint file created by PyTorch Lightning, a PyTorch research framework. It contains a dump of a PyTorch Lightning machine learning model. Developers create CKPT files to preserve the previous states of a machine learning model, while training it to its final state.\n",
    "    + [pytorch lightning](https://github.com/Lightning-AI/pytorch-lightning)\n",
    "* If it doesn't have `safetensors`/`ckpt`, that means it is only available as diffuser (you can convert it to `ckpt`)\n",
    "  - This is a lengthy video about converting to a `ckpt` https://www.youtube.com/watch?v=-6CA18MS0pY\n",
    "  - IIRC, ShivamShrirao's dreambooth colab also have a section to convert diffuser weight to `ckpt`\n",
    "* Save files to cache (`~/.cache/huggingface/hub`), you can read more about it [here](https://huggingface.co/docs/transformers/main/en/installation#cache-setup)\n",
    "* The model is available for loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Use .safetensors Model Files In Stable Diffusion WebUI](https://techtactician.com/ckpt-vs-safetensors-in-stable-diffusion/)\n",
    "* [Convert Stable Diffusion Diffusers (.bin Weights) & Dreambooth Models to CKPT File](https://www.youtube.com/watch?v=-6CA18MS0pY)\n",
    "* [Discussion board](https://www.reddit.com/r/StableDiffusion/comments/12djqlh/please_help_an_idiot_understand_how_to_download/)\n",
    "* [](https://www.kaggle.com/code/shravankumar147/save-huggingface-model-to-local-for-no-internet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
